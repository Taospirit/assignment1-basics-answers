import torch
from torch import nn
from einops import rearrange, reduce, einsum


def _init_weights(in_features, out_features, device, dtype):
    w = torch.empty(out_features, in_features, device=device, dtype=dtype)
    std = (2 / (in_features + out_features)) ** 0.5
    nn.init.trunc_normal_(w, mean=0.0, std=std, a=-3 * std, b=3 * std)
    w = nn.Parameter(w)
    return w


def _init_embedding(num_embeddings, embedding_dim, device, dtype):
    w = torch.empty(num_embeddings, embedding_dim, device=device, dtype=dtype)
    nn.init.trunc_normal_(w, mean=0.0, std=1, a=-3, b=3)
    w = nn.Parameter(w)
    return w


def softmax_impl(x: torch.Tensor, dim: int) -> torch.Tensor:
    x = x - torch.max(x, dim=dim, keepdim=True)[0]
    x = torch.exp(x)
    x_sum = torch.sum(x, dim=dim, keepdim=True)
    # Add small epsilon to prevent division by zero
    x_sum = torch.clamp(x_sum, min=1e-12)
    x = x / x_sum
    return x


def scaled_dot_product_attention_impl(
    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None
) -> torch.Tensor:
    """
    Q: [..., seq_len_q, d_k]
    K: [..., seq_len_k, d_k]
    V: [..., seq_len_v, d_v]
    mask: [..., seq_len_q, secq_len_k]
    """
    d_k = Q.shape[-1]
    QK = einsum(
        Q, K, "... seq_q d_k, ... seq_k d_k -> ... seq_q seq_k"
    )  # [..., seq_len_q, seq_len_k]
    QK = QK / (d_k**0.5)
    if mask is not None:
        QK = QK.masked_fill(mask == 0, float("-inf"))
    QK = softmax_impl(QK, dim=-1)
    # einsum is so awesome!!!
    return einsum(
        QK, V, "... seq_q seq_k, ... seq_k d_v -> ... seq_q d_v"
    )  # [..., seq_len_q, d_v]


class Linear(nn.Module):
    """Implementing the linear module"""

    def __init__(self, in_features, out_features, device=None, dtype=None):
        # Construct a linear transformation module. This function should accept the following parameters:
        # in_features: int final dimension of the input
        # out_features: int final dimension of the output
        # device: torch.device | None = None Device to store the parameters on
        # dtype: torch.dtype | None = None Data type of the parameters
        super().__init__()
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32
        # w in shape [d_out, d_in]
        self.weights = _init_weights(in_features, out_features, self.device, self.dtype)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Apply the linear transformation to the input.
        # x in shape [..., d_in]
        return x @ self.weights.T

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.weights.data = state_dict["weights"]


class Embedding(nn.Module):
    """Implementing the embedding module"""

    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):
        # Construct an embedding module. This function should accept the following parameters:
        # num_embeddings: int Size of the vocabulary
        # embedding_dim: int Dimension of the embedding
        # device: torch.device | None = None Device to store the parameters on
        # dtype: torch.dtype | None = None Data type of the parameters
        super().__init__()
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32
        self.weights = _init_embedding(
            num_embeddings, embedding_dim, self.device, self.dtype
        )

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # Lookup the embedding vectors for the given token IDs.
        # The forward method should select the embedding vector for each token ID
        # by indexing into an embedding matrix of shape (vocab_size, d_model)
        # using a torch.LongTensor of token IDs with shape (batch_size, sequence_length).
        token_ids = token_ids.to(torch.long)
        return self.weights[token_ids]

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.weights.data = state_dict["weights"]


class RMSNorm(nn.Module):
    """Implementing the RMNorm module"""

    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):
        # Construct the RMSNorm module. This function should accept the following parameters:
        # d_model: int Hidden dimension of the model
        # eps: float = 1e-5 Epsilon value for numerical stability
        # device: torch.device | None = None Device to store the parameters on
        # dtype: torch.dtype | None = None Data type of the parameters
        super().__init__()
        self.eps = eps
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32
        self.weights = nn.Parameter(
            torch.ones(d_model, device=self.device, dtype=self.dtype)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Process an input tensor of shape (batch_size, sequence_length, d_model)
        # and return a tensor of the same shape.
        in_dtype = x.dtype
        x = x.to(torch.float32)
        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)
        x = x / rms * self.weights
        x = x.to(in_dtype)
        return x

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.weights.data = state_dict["weights"]


class SwiGLU(nn.Module):
    """Implementing the SwiGLU module"""

    def __init__(self, d_model: int, d_ff: int = None, device=None, dtype=None):
        # Construct the SwiGLU module. This function should accept the following parameters:
        # d_model: int Hidden dimension of the model
        # d_ff: int Intermediate dimension of the model
        # device: torch.device | None = None Device to store the parameters on
        # dtype: torch.dtype | None = None Data type of the parameters
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff if d_ff is not None else int(8 / 3 * d_model)
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32

        # match the shape of test setting
        self.w1 = _init_weights(self.d_model, self.d_ff, self.device, self.dtype)
        self.w2 = _init_weights(self.d_ff, self.d_model, self.device, self.dtype)
        self.w3 = _init_weights(self.d_model, self.d_ff, self.device, self.dtype)
        self.silu = lambda x: x * torch.sigmoid(x)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Process an input tensor of shape (batch_size, sequence_length, d_model)
        # and return a tensor of the same shape.
        # W2(SiLU(W1x) * W3x)
        return (self.silu(x @ self.w1.T) * (x @ self.w3.T)) @ self.w2.T

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.w1.data = state_dict["w1"]
        self.w2.data = state_dict["w2"]
        self.w3.data = state_dict["w3"]


class RoPE(nn.Module):
    """Implementing the RoPE module"""

    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):
        # Construct the RoPE module and create buffers if needed.
        # theta: float Î˜ value for the RoPE
        # d_k: int dimension of query and key vectors
        # max_seq_len: int Maximum sequence length that will be inputted
        # device: torch.device | None = None Device to store the buffer on
        super().__init__()
        assert d_k % 2 == 0, "RoPE requires even dimension d_k"
        self.d_k = d_k
        self.device = device if device is not None else torch.device("cpu")
        # precompute the frequency sequence
        # æ ¹æ® RoPE è®ºæ–‡ï¼Œé¢‘ç‡åº”è¯¥æ˜¯ theta^(-2i/d) å…¶ä¸­ i = 0, 1, ..., d/2-1
        half_dim = self.d_k // 2
        freq_seq = torch.arange(0, half_dim, device=self.device)
        inv_freq = 1.0 / (theta ** (freq_seq * 2.0 / self.d_k))
        # i * theta^(-2i/d)
        t = torch.arange(max_seq_len, device=self.device)  # [max_seq_len]
        freqs = einsum(
            t, inv_freq, "seq_len, half_dim -> seq_len half_dim"
        )  # [max_seq_len, d_k/2]

        # æ³¨å†Œ bufferï¼ˆä¸éœ€è¦æŒä¹…åŒ–åˆ° checkpointï¼‰
        self.register_buffer(
            "cos_cached", freqs.cos(), persistent=False
        )  # [max_seq_len, d_k/2]
        self.register_buffer(
            "sin_cached", freqs.sin(), persistent=False
        )  # [max_seq_len, d_k/2]

    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:
        # Process an input tensor of shape (..., seq_len, d_k) and return a tensor of the same shape.
        # Note that you should tolerate x with an arbitrary number of batch dimensions. You should
        # assume that the token positions are a tensor of shape (..., seq_len) specifying the token
        # positions of x along the sequence dimension.
        # You should use the token positions to slice your (possibly precomputed) cos and sin tensors
        # along the sequence dimension.
        *batch_dims, seq_len, d_k = x.shape
        assert d_k == self.d_k

        # æ ¹æ® token_positions è·å–å¯¹åº”çš„ cos å’Œ sin å€¼
        # cos_cached å’Œ sin_cached çš„å½¢çŠ¶æ˜¯ [max_seq_len, d_k/2]
        cos = self.cos_cached[token_positions]  # [..., seq_len, d_k/2]
        sin = self.sin_cached[token_positions]  # [..., seq_len, d_k/2]

        # æ‹†åˆ†è¾“å…¥ä¸ºå¶æ•°ç»´å’Œå¥‡æ•°ç»´
        x1 = x[..., ::2]  # [..., seq_len, d_k/2] - å¶æ•°ç´¢å¼•
        x2 = x[..., 1::2]  # [..., seq_len, d_k/2] - å¥‡æ•°ç´¢å¼•

        # åº”ç”¨æ—‹è½¬å˜æ¢: [cos*x1 - sin*x2, sin*x1 + cos*x2]
        x1_rot = x1 * cos - x2 * sin  # [..., seq_len, d_k/2]
        x2_rot = x1 * sin + x2 * cos  # [..., seq_len, d_k/2]

        # é‡æ–°äº¤é”™ç»„åˆå›åŸå§‹å½¢çŠ¶
        result = torch.zeros_like(x)
        result[..., ::2] = x1_rot  # å¶æ•°ç´¢å¼•
        result[..., 1::2] = x2_rot  # å¥‡æ•°ç´¢å¼•

        return result


class MultiHeadAttention(nn.Module):
    """Implementing the MultiHeadAttention module"""

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        theta: float = None,
        max_seq_len: int = None,
        token_positions: torch.Tensor = None,
        device=None,
        dtype=None,
    ):
        # Construct the MultiHeadAttention module. This function should accept the following parameters:
        # d_model: int Hidden dimension of the model
        # num_heads: int Number of attention heads
        # device: torch.device | None = None Device to store the parameters on
        # dtype: torch.dtype | None = None Data type of the parameters
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32

        self.q_proj = _init_weights(d_model, d_model, self.device, self.dtype)
        self.k_proj = _init_weights(d_model, d_model, self.device, self.dtype)
        self.v_proj = _init_weights(d_model, d_model, self.device, self.dtype)
        self.o_proj = _init_weights(d_model, d_model, self.device, self.dtype)
        # slice the d_model into num_heads heads
        self.d_k = d_model // num_heads
        self.d_v = d_model // num_heads

        if theta is not None and max_seq_len is not None:
            self.RoPE = RoPE(theta, self.d_k, max_seq_len, self.device)
        else:
            self.RoPE = None
        self.token_positions = token_positions

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Process an input tensor of shape (batch_size, sequence_length, d_model)
        # and return a tensor of the same shape.
        # Note that you should tolerate x with an arbitrary number of batch dimensions. You should
        # assume that the token positions are a tensor of shape (..., sequence_length) specifying the token
        # positions of x along the sequence dimension.
        # You should use the token positions to slice your (possibly precomputed) cos and sin tensors
        # along the sequence dimension.
        *batch_dims, seq_len, d_model = x.shape
        assert (
            d_model == self.d_model
        ), f"d_model not match: {d_model} != {self.d_model}"

        # å¦‚æœæ²¡æœ‰æä¾› token_positionsï¼Œåˆ™ç”Ÿæˆé»˜è®¤çš„åºåˆ—ä½ç½® [0, 1, 2, ..., seq_len-1]
        if self.token_positions is None:
            token_positions = torch.arange(seq_len, device=x.device, dtype=torch.long)
            # æ‰©å±•åˆ°åŒ¹é…æ‰¹æ¬¡ç»´åº¦
            for _ in batch_dims:
                token_positions = token_positions.unsqueeze(0)
            token_positions = token_positions.expand(*batch_dims, seq_len)
        else:
            token_positions = self.token_positions
        # project the input to q, k, v
        q = x @ self.q_proj.T  # [..., seq_len, d_model] -> [..., seq_len, d_model]
        k = x @ self.k_proj.T  # [..., seq_len, d_model] -> [..., seq_len, d_model]
        v = x @ self.v_proj.T  # [..., seq_len, d_model] -> [..., seq_len, d_model]

        # Split heads using rearrange for better readability
        q = rearrange(
            q,
            "... seq_len (num_heads d_k) -> ... num_heads seq_len d_k",
            num_heads=self.num_heads,
            d_k=self.d_k,
        )
        k = rearrange(
            k,
            "... seq_len (num_heads d_k) -> ... num_heads seq_len d_k",
            num_heads=self.num_heads,
            d_k=self.d_k,
        )
        v = rearrange(
            v,
            "... seq_len (num_heads d_k) -> ... num_heads seq_len d_k",
            num_heads=self.num_heads,
            d_k=self.d_k,
        )

        # apply RoPE to q and k
        if self.RoPE is not None:
            q = self.RoPE(q, token_positions)
            k = self.RoPE(k, token_positions)

        # apply scaled dot product attention
        # Create causal mask: 1 for allowed positions, 0 for masked positions
        mask = torch.tril(
            torch.ones(seq_len, seq_len, device=self.device, dtype=torch.bool),
            diagonal=0,
        )
        attn = scaled_dot_product_attention_impl(q, k, v, mask)

        # Concatenate heads using rearrange
        attn = rearrange(
            attn, "... num_heads seq_len d_k -> ... seq_len (num_heads d_k)"
        )
        attn = attn @ self.o_proj.T
        return attn

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.q_proj.data = state_dict["q_proj"]
        self.k_proj.data = state_dict["k_proj"]
        self.v_proj.data = state_dict["v_proj"]
        self.o_proj.data = state_dict["o_proj"]


class TransformerBlock(nn.Module):
    """Implementing the TransformerBlock module"""

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        d_ff: int,
        theta: float,
        max_seq_len: int,
        device=None,
        dtype=None,
    ):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32

        self.rms_norm1 = RMSNorm(d_model, device=self.device, dtype=self.dtype)
        self.multihead_attention = MultiHeadAttention(
            d_model,
            num_heads,
            theta,
            max_seq_len,
            device=self.device,
            dtype=self.dtype,
        )
        self.rms_norm2 = RMSNorm(d_model, device=self.device, dtype=self.dtype)
        self.swiglu = SwiGLU(d_model, d_ff, device=self.device, dtype=self.dtype)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        feat1 = self.rms_norm1(x)
        feat1 = self.multihead_attention(feat1)
        feat1 = x + feat1

        feat2 = self.rms_norm2(feat1)
        feat2 = self.swiglu(feat2)
        feat2 = feat1 + feat2

        return feat2

    def load_state_dict(self, state_dict: dict[str, torch.Tensor]):
        self.rms_norm1.load_state_dict({"weights": state_dict["ln1.weight"]})
        self.multihead_attention.load_state_dict(
            {
                "q_proj": state_dict["attn.q_proj.weight"],
                "k_proj": state_dict["attn.k_proj.weight"],
                "v_proj": state_dict["attn.v_proj.weight"],
                "o_proj": state_dict["attn.output_proj.weight"],
            }
        )
        self.rms_norm2.load_state_dict({"weights": state_dict["ln2.weight"]})
        self.swiglu.load_state_dict(
            {
                "w1": state_dict["ffn.w1.weight"],
                "w2": state_dict["ffn.w2.weight"],
                "w3": state_dict["ffn.w3.weight"],
            }
        )


class TransformerLM(nn.Module):
    """Implementing the TransformerLM module"""

    def __init__(
        self,
        vocab_size: int,
        context_length: int,
        d_model: int,
        num_layers: int,
        num_heads: int,
        d_ff: int,
        rope_theta: float,
        device=None,
        dtype=None,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.context_length = context_length
        self.d_model = d_model
        self.num_layers = num_layers
        self.device = device if device is not None else torch.device("cpu")
        self.dtype = dtype if dtype is not None else torch.float32

        self.embedding = Embedding(
            vocab_size, d_model, device=self.device, dtype=self.dtype
        )
        self.transformer_blocks = nn.ModuleList(
            [
                TransformerBlock(
                    d_model,
                    num_heads,
                    d_ff,
                    rope_theta,
                    context_length,
                    device=self.device,
                    dtype=self.dtype,
                )
                for _ in range(num_layers)
            ]
        )
        self.rms_norm = RMSNorm(d_model, device=self.device, dtype=self.dtype)
        self.lm_head = Linear(d_model, vocab_size, device=self.device, dtype=self.dtype)

    def forward(self, in_indices: torch.Tensor) -> torch.Tensor:
        # in_indices (Int[Tensor, "batch_size sequence_length"])
        x = self.embedding(in_indices)
        for block in self.transformer_blocks:
            x = block(x)
        x = self.rms_norm(x)
        x = self.lm_head(x)
        return x


def calculate_flops_manual(
    vocab_size: int,
    context_length: int,
    d_model: int,
    num_layers: int,
    num_heads: int,
    d_ff: int,
) -> dict:
    """
    æ‰‹åŠ¨è®¡ç®— Transformer æ¨¡å‹çš„ FLOPs (æµ®ç‚¹è¿ç®—æ¬¡æ•°)

    è¿™ä¸ªå‡½æ•°é€šè¿‡æ•°å­¦å…¬å¼ç²¾ç¡®è®¡ç®—æ¯ä¸ªç»„ä»¶çš„è®¡ç®—å¤æ‚åº¦ï¼Œé¿å…äº†
    è‡ªåŠ¨åˆ†æå·¥å…·çš„æ€§èƒ½å¼€é”€ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§æ¨¡å‹çš„ FLOPs ä¼°ç®—ã€‚

    Args:
        vocab_size: è¯æ±‡è¡¨å¤§å°
        context_length: ä¸Šä¸‹æ–‡é•¿åº¦ (æœ€å¤§åºåˆ—é•¿åº¦)
        d_model: æ¨¡å‹éšè—ç»´åº¦
        num_layers: Transformer å±‚æ•°
        num_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
        d_ff: å‰é¦ˆç½‘ç»œçš„éšè—ç»´åº¦

    Returns:
        åŒ…å«å„ç»„ä»¶ FLOPs è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
    """
    # ç®€åŒ–å‡è®¾ï¼šbatch_size = 1ï¼Œå®é™…åºåˆ—é•¿åº¦ = context_length
    batch_size, seq_len = 1, context_length
    d_k = d_model // num_heads  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦

    # ================================
    # 1. Token Embedding å±‚
    # ================================
    # åµŒå…¥å±‚æ˜¯æŸ¥è¡¨æ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•ï¼ŒFLOPs = 0
    embedding_flops = 0

    # ================================
    # 2. å•ä¸ª Transformer å±‚çš„ FLOPs
    # ================================

    # --------------------------------
    # 2.1 Multi-Head Self-Attention
    # --------------------------------

    # 2.1.1 QKV çº¿æ€§æŠ•å½±
    # è¾“å…¥: [batch_size, seq_len, d_model] -> è¾“å‡º: [batch_size, seq_len, d_model]
    # æ¯ä¸ªæŠ•å½± (Q/K/V): çŸ©é˜µä¹˜æ³• (seq_len, d_model) @ (d_model, d_model) = 2 * seq_len * d_model * d_model FLOPs
    # æ€»å…± 3 ä¸ªæŠ•å½± (Q, K, V)
    qkv_proj_flops = 3 * 2 * seq_len * d_model * d_model

    # 2.1.2 æ³¨æ„åŠ›åˆ†æ•°è®¡ç®— (Q @ K^T)
    # Q: [batch_size, num_heads, seq_len, d_k]
    # K^T: [batch_size, num_heads, d_k, seq_len]
    # Q @ K^T: [batch_size, num_heads, seq_len, seq_len]
    # æ¯ä¸ªå¤´çš„è®¡ç®—: çŸ©é˜µä¹˜æ³• (seq_len, d_k) @ (d_k, seq_len) = 2 * seq_len * d_k * seq_len FLOPs
    # æ‰€æœ‰å¤´: num_heads Ã— 2 Ã— seq_len Ã— seq_len Ã— d_k
    attention_scores_flops = num_heads * 2 * seq_len * seq_len * d_k

    # 2.1.3 Softmax æ“ä½œ
    # å¯¹æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ [seq_len, seq_len] çŸ©é˜µè¿›è¡Œ softmax
    # ä¸»è¦è®¡ç®—: æŒ‡æ•°è¿ç®— + æ±‚å’Œ + é™¤æ³•
    # è¿‘ä¼¼ä¼°ç®—: num_heads Ã— seq_len Ã— seq_len æ¬¡è¿ç®—
    softmax_flops = num_heads * seq_len * seq_len

    # 2.1.4 æ³¨æ„åŠ›åŠ æƒ (Attention @ V)
    # Attention: [batch_size, num_heads, seq_len, seq_len]
    # V: [batch_size, num_heads, seq_len, d_k]
    # è¾“å‡º: [batch_size, num_heads, seq_len, d_k]
    # æ¯ä¸ªå¤´çš„è®¡ç®—: çŸ©é˜µä¹˜æ³• (seq_len, seq_len) @ (seq_len, d_k) = 2 * seq_len * seq_len * d_k FLOPs
    attention_weighted_flops = num_heads * 2 * seq_len * seq_len * d_k

    # 2.1.5 è¾“å‡ºçº¿æ€§æŠ•å½±
    # è¾“å…¥: [batch_size, seq_len, d_model] -> è¾“å‡º: [batch_size, seq_len, d_model]
    # çŸ©é˜µä¹˜æ³• (seq_len, d_model) @ (d_model, d_model) = 2 * seq_len * d_model * d_model FLOPs
    output_proj_flops = 2 * seq_len * d_model * d_model

    # æ³¨æ„åŠ›æœºåˆ¶æ€» FLOPs
    attention_total_flops = (
        qkv_proj_flops  # QKV æŠ•å½±
        + attention_scores_flops  # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—
        + softmax_flops  # Softmax å½’ä¸€åŒ–
        + attention_weighted_flops  # æ³¨æ„åŠ›åŠ æƒ
        + output_proj_flops  # è¾“å‡ºæŠ•å½±
    )

    # --------------------------------
    # 2.2 Feed Forward Network (SwiGLU)
    # --------------------------------
    # SwiGLU ç»“æ„: x -> [W1(x), W3(x)] -> [SiLU(W1(x)), W3(x)] -> SiLU(W1(x)) âŠ™ W3(x) -> W2(...)

    # 2.2.1 ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ W1: [seq_len, d_model] @ [d_model, d_ff] = [seq_len, d_ff]
    # çŸ©é˜µä¹˜æ³• FLOPs = 2 * seq_len * d_model * d_ff
    w1_flops = 2 * seq_len * d_model * d_ff

    # 2.2.2 ç¬¬ä¸‰ä¸ªçº¿æ€§å±‚ W3: [seq_len, d_model] @ [d_model, d_ff] = [seq_len, d_ff]
    # çŸ©é˜µä¹˜æ³• FLOPs = 2 * seq_len * d_model * d_ff
    w3_flops = 2 * seq_len * d_model * d_ff

    # 2.2.3 SiLU æ¿€æ´»å‡½æ•°: SiLU(x) = x * sigmoid(x)
    # éœ€è¦è®¡ç®— sigmoid + å…ƒç´ ä¹˜æ³•ï¼Œè¿‘ä¼¼ seq_len * d_ff æ¬¡è¿ç®—
    silu_flops = seq_len * d_ff

    # 2.2.4 å…ƒç´ çº§ä¹˜æ³•: SiLU(W1(x)) âŠ™ W3(x)
    # seq_len * d_ff æ¬¡å…ƒç´ ä¹˜æ³•
    elementwise_multiply_flops = seq_len * d_ff

    # 2.2.5 ç¬¬äºŒä¸ªçº¿æ€§å±‚ W2: [seq_len, d_ff] @ [d_ff, d_model] = [seq_len, d_model]
    # çŸ©é˜µä¹˜æ³• FLOPs = 2 * seq_len * d_ff * d_model
    w2_flops = 2 * seq_len * d_ff * d_model

    # å‰é¦ˆç½‘ç»œæ€» FLOPs
    ffn_flops = w1_flops + w3_flops + silu_flops + elementwise_multiply_flops + w2_flops
    # ç®€åŒ–è¡¨è¾¾å¼: 2Ã—(W1+W3) + 2Ã—W2 + æ¿€æ´» = 2Ã—2Ã—seq_lenÃ—d_modelÃ—d_ff + 2Ã—seq_lenÃ—d_ffÃ—d_model + 2Ã—seq_lenÃ—d_ff
    # = seq_len Ã— (4Ã—d_modelÃ—d_ff + 2Ã—d_ffÃ—d_model + 2Ã—d_ff) = seq_len Ã— (6Ã—d_modelÃ—d_ff + 2Ã—d_ff)
    ffn_flops = seq_len * (6 * d_model * d_ff + 2 * d_ff)

    # --------------------------------
    # 2.3 RMS Norm å±‚ (æ¯ä¸ª Transformer å±‚æœ‰ 2 ä¸ª)
    # --------------------------------
    # RMS Norm è®¡ç®—è¿‡ç¨‹:
    # 1. è®¡ç®—å¹³æ–¹: x^2 (seq_len * d_model æ¬¡ä¹˜æ³•)
    # 2. è®¡ç®—å‡å€¼: mean(x^2) (seq_len * d_model æ¬¡åŠ æ³• + 1 æ¬¡é™¤æ³•)
    # 3. è®¡ç®—å¹³æ–¹æ ¹: sqrt(mean(x^2) + Îµ) (1 æ¬¡å¹³æ–¹æ ¹è¿ç®—)
    # 4. å½’ä¸€åŒ–: x / sqrt(...) (seq_len * d_model æ¬¡é™¤æ³•)
    # 5. ç¼©æ”¾: normalized_x * weight (seq_len * d_model æ¬¡ä¹˜æ³•)
    #
    # æ€»è®¡: æ¯ä¸ª RMS Norm çº¦ 3 Ã— seq_len Ã— d_model æ¬¡è¿ç®—
    # æ¯å±‚æœ‰ 2 ä¸ª RMS Norm (attention å‰åå„ä¸€ä¸ª)
    rms_norm_flops = 2 * seq_len * d_model * 3

    # --------------------------------
    # 2.4 å•å±‚æ€»è®¡
    # --------------------------------
    layer_flops = attention_total_flops + ffn_flops + rms_norm_flops

    # ================================
    # 3. æ‰€æœ‰ Transformer å±‚
    # ================================
    all_layers_flops = num_layers * layer_flops

    # ================================
    # 4. æœ€ç»ˆ RMS Norm
    # ================================
    # åœ¨æ‰€æœ‰ Transformer å±‚ä¹‹åè¿˜æœ‰ä¸€ä¸ª RMS Norm
    final_norm_flops = seq_len * d_model * 3

    # ================================
    # 5. Language Model Head
    # ================================
    # çº¿æ€§æŠ•å½±: [seq_len, d_model] @ [d_model, vocab_size] = [seq_len, vocab_size]
    # çŸ©é˜µä¹˜æ³• FLOPs = 2 * seq_len * d_model * vocab_size
    lm_head_flops = 2 * seq_len * d_model * vocab_size

    # ================================
    # 6. æ€»è®¡ç®—é‡
    # ================================
    total_flops = (
        embedding_flops  # Token Embedding (0)
        + all_layers_flops  # æ‰€æœ‰ Transformer å±‚
        + final_norm_flops  # æœ€ç»ˆ RMS Norm
        + lm_head_flops  # Language Model Head
    )

    return {
        "embedding_flops": embedding_flops,
        "attention_flops_per_layer": attention_total_flops,
        "ffn_flops_per_layer": ffn_flops,
        "rms_norm_flops_per_layer": rms_norm_flops,
        "layer_flops": layer_flops,
        "all_layers_flops": all_layers_flops,
        "final_norm_flops": final_norm_flops,
        "lm_head_flops": lm_head_flops,
        "total_flops": total_flops,
        # é¢å¤–çš„è¯¦ç»†ä¿¡æ¯
        "qkv_proj_flops": qkv_proj_flops,
        "attention_scores_flops": attention_scores_flops,
        "softmax_flops": softmax_flops,
        "attention_weighted_flops": attention_weighted_flops,
        "output_proj_flops": output_proj_flops,
        "w1_flops": w1_flops,
        "w3_flops": w3_flops,
        "silu_flops": silu_flops,
        "elementwise_multiply_flops": elementwise_multiply_flops,
        "w2_flops": w2_flops,
    }


if __name__ == "__main__":
    from fvcore.nn import FlopCountAnalysis, parameter_count_table

    # GPT2-XL å‚æ•°é…ç½®
    model = TransformerLM(
        vocab_size=50257,
        context_length=1024,
        d_model=1600,
        num_layers=48,
        num_heads=25,
        d_ff=6400,
        rope_theta=10000,
    )
    print(f"model init ok")

    # in_indices = torch.randint(0, 50257, (1, 1024))
    # print(f"in_indices shape: {in_indices.shape}")

    # åªæµ‹è¯•å‰å‘ä¼ æ’­ï¼Œè·³è¿‡è€—æ—¶çš„ FLOPs åˆ†æ
    # print("å¼€å§‹å‰å‘ä¼ æ’­æµ‹è¯•...")
    # try:
    #     with torch.no_grad():
    #         output = model(in_indices)
    #     print(f"å‰å‘ä¼ æ’­æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {output.shape}")
    # except Exception as e:
    #     print(f"å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    #     exit(1)

    try:
        print(f"Parameter count: {parameter_count_table(model)}")
    except Exception as e:
        print(f"å‚æ•°ç»Ÿè®¡å¤±è´¥: {e}")

    # æ‰‹åŠ¨è®¡ç®—å„ç»„ä»¶çš„ FLOPs
    print("\n=== æ‰‹åŠ¨è®¡ç®— FLOPs (è¯¦ç»†åˆ†è§£) ===")

    # è®¡ç®—å½“å‰æ¨¡å‹çš„ FLOPs
    flops_breakdown = calculate_flops_manual(
        vocab_size=50257,
        context_length=1024,
        d_model=1600,
        num_layers=48,
        num_heads=25,
        d_ff=6400,
    )

    # ä¸»è¦ç»„ä»¶ç»Ÿè®¡
    print("ğŸ“Š ä¸»è¦ç»„ä»¶ FLOPs ç»Ÿè®¡:")
    print(f"  Token Embedding FLOPs: {flops_breakdown['embedding_flops']:,}")
    print(f"  æ¯å±‚æ³¨æ„åŠ›æœºåˆ¶ FLOPs: {flops_breakdown['attention_flops_per_layer']:,}")
    print(f"  æ¯å±‚å‰é¦ˆç½‘ç»œ FLOPs: {flops_breakdown['ffn_flops_per_layer']:,}")
    print(f"  æ¯å±‚ RMS Norm FLOPs: {flops_breakdown['rms_norm_flops_per_layer']:,}")
    print(f"  å•å±‚æ€» FLOPs: {flops_breakdown['layer_flops']:,}")
    print(f"  æ‰€æœ‰ {48} å±‚ FLOPs: {flops_breakdown['all_layers_flops']:,}")
    print(f"  æœ€ç»ˆ RMS Norm FLOPs: {flops_breakdown['final_norm_flops']:,}")
    print(f"  Language Model Head FLOPs: {flops_breakdown['lm_head_flops']:,}")

    print(f"\nğŸ¯ æ€» FLOPs: {flops_breakdown['total_flops']:,}")
    print(f"ğŸ¯ æ€» FLOPs (ç§‘å­¦è®¡æ•°æ³•): {flops_breakdown['total_flops']:.2e}")

    # è¯¦ç»†åˆ†è§£ (æ³¨æ„åŠ›æœºåˆ¶)
    print(f"\nğŸ” æ³¨æ„åŠ›æœºåˆ¶è¯¦ç»†åˆ†è§£:")
    print(f"  QKV æŠ•å½±: {flops_breakdown['qkv_proj_flops']:,}")
    print(f"  æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—: {flops_breakdown['attention_scores_flops']:,}")
    print(f"  Softmax æ“ä½œ: {flops_breakdown['softmax_flops']:,}")
    print(f"  æ³¨æ„åŠ›åŠ æƒ: {flops_breakdown['attention_weighted_flops']:,}")
    print(f"  è¾“å‡ºæŠ•å½±: {flops_breakdown['output_proj_flops']:,}")

    # è¯¦ç»†åˆ†è§£ (å‰é¦ˆç½‘ç»œ)
    print(f"\nğŸ” å‰é¦ˆç½‘ç»œ (SwiGLU) è¯¦ç»†åˆ†è§£:")
    print(f"  W1 çº¿æ€§å±‚: {flops_breakdown['w1_flops']:,}")
    print(f"  W3 çº¿æ€§å±‚: {flops_breakdown['w3_flops']:,}")
    print(f"  SiLU æ¿€æ´»: {flops_breakdown['silu_flops']:,}")
    print(f"  å…ƒç´ ä¹˜æ³•: {flops_breakdown['elementwise_multiply_flops']:,}")
    print(f"  W2 çº¿æ€§å±‚: {flops_breakdown['w2_flops']:,}")

    print("\næ¨¡å‹æµ‹è¯•å®Œæˆ!")
